{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# 设置设备（如果有GPU则使用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载 ray 数据的函数\n",
    "def load_ray_data(filename, sizes):\n",
    "    \"\"\"\n",
    "    Loads ray data from a binary file.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename (str): Path to the binary file containing ray data.\n",
    "    \n",
    "    Returns:\n",
    "    - torch.Tensor: Ray data tensor.\n",
    "    \"\"\"\n",
    "    stdata = np.fromfile(filename, dtype=np.float32)\n",
    "    stdata = stdata.reshape(sizes)\n",
    "    stdata_pos = np.sum(stdata, axis=(-2, -1))\n",
    "    stdata_pos = stdata_pos / np.sum(stdata_pos) * sizes[0] * sizes[1] / math.pi / 4\n",
    "    stdata_pos = torch.tensor(stdata_pos, dtype=torch.float, device=device)\n",
    "    return stdata_pos.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# 定义残差块\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False)  # 添加 1x1 卷积层\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "        # 可学习参数 alpha，用于增强残差连接\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out = self.alpha * out + identity  # 残差连接带有可学习权重\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# 定义 EncoderResNet 模型，添加 Attention 机制\n",
    "class EncoderResNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=64):\n",
    "        super(EncoderResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # 初始卷积层，使用较小的卷积核，并添加残差块\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=1, stride=1, bias=False),  # 添加 1x1 卷积层来增强信息\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 使用 MaxPooling 来提取高频信息\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # 使用卷积代替池化层\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # 残差块层定义\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "\n",
    "        # Attention 机制\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)\n",
    "        \n",
    "\n",
    "        # Attention Pooling \n",
    "        self.attention_pooling = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=1, bias=False),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(512, embedding_dim)\n",
    "\n",
    "    def _make_layer(self, out_channels, blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # Attention 机制\n",
    "        b, c, h, w = x.size()\n",
    "        x = x.view(b, c, -1).permute(0, 2, 1)  # (batch_size, num_patches, channels)\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        x = x.permute(0, 2, 1).view(b, c, h, w)  # (batch_size, channels, height, width)\n",
    "\n",
    "        # Attention Pooling\n",
    "        x = self.attention_pooling(x)\n",
    "        x = torch.sum(x, dim=(-2, -1))  # 将特征图加权求和，得到全局特征\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# SphereGaussianMixture 解码器模块定义\n",
    "class SphereGaussianMixture(nn.Module):\n",
    "    def __init__(self, embedding_dim=64, hidden_dim=128, num_spheres=32, dropout_rate=0.2):\n",
    "        super(SphereGaussianMixture, self).__init__()\n",
    "        self.num_spheres = num_spheres\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, num_spheres * 4)  # 4 parameters per sphere\n",
    "        )\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        # Mixture Weights\n",
    "        out = self.fc(embedding)  # Shape: (batch_size, num_spheres * 4)\n",
    "        weights = F.softmax(out[:, :self.num_spheres], dim=-1)  # Shape: (batch_size, num_spheres)\n",
    "\n",
    "        # Convert theta and phi to angles in radians\n",
    "        theta_phi = torch.sigmoid(out[:, self.num_spheres: 3 * self.num_spheres])\n",
    "        theta = theta_phi[:, :self.num_spheres] * math.pi\n",
    "        phi = theta_phi[:, self.num_spheres:] * 2 * math.pi\n",
    "\n",
    "        # Calculate axes using spherical coordinates\n",
    "        cos_theta = torch.cos(theta)\n",
    "        sin_theta = torch.sin(theta)\n",
    "        cos_phi = torch.cos(phi)\n",
    "        sin_phi = torch.sin(phi)\n",
    "        axes = torch.stack((cos_theta, sin_theta * cos_phi, sin_theta * sin_phi), dim=-1)  # Shape: (batch_size, num_spheres, 3)\n",
    "\n",
    "        # Regularize kappa with clamping to avoid numerical instability\n",
    "        kappas = torch.exp(out[:, 3 * self.num_spheres:])  # Shape: (batch_size, num_spheres)\n",
    "        kappas = torch.clamp(kappas, max=100)  # Clamp kappa for stability\n",
    "\n",
    "        return weights.squeeze(), axes.squeeze(), kappas.squeeze()\n",
    "\n",
    "def vmf_pdf(x, axes, kappas):\n",
    "    # Ensure kappas are non-negative for stability\n",
    "    # kappas = torch.clamp(kappas, min=1e-10, max=1e5)\n",
    "\n",
    "    # Define thresholds for approximations\n",
    "    large_kappa_threshold = 1e5  # Threshold for considering kappa as \"large\"\n",
    "    small_kappa_threshold = 1e-3  # Threshold for considering kappa as \"small\"\n",
    "\n",
    "    # Approximate normalization constant for large and small kappa values\n",
    "\n",
    "\n",
    "    norm_const = torch.where(\n",
    "        kappas > large_kappa_threshold,\n",
    "        kappas / (2 * math.pi),  # Approximation for large kappa\n",
    "        kappas / (2 * math.pi * (1-torch.exp(-2*kappas)))\n",
    "    )\n",
    "    # norm_const = kappas / (4 * math.pi * (1-torch.exp(-2*kappas)))\n",
    "\n",
    "    # Compute dot products between input w and the axes of the spheres (unit vectors)\n",
    "    dot_products = torch.matmul(x, axes.transpose(0, 1))-1  # Shape: (data_sizes, num_spheres)\n",
    "\n",
    "    # # Compute von Mises-Fisher pdf values\n",
    "    return norm_const * torch.exp(kappas * dot_products)\n",
    "\n",
    "# 多重 von Mises-Fisher 分布函数\n",
    "def multi_vmf(weights, axes, kappas, w):\n",
    "    # Ensure kappas are non-negative for stability\n",
    "    kappas = torch.clamp(kappas, min=1e-10, max=1e5)\n",
    "\n",
    "    # Define thresholds for approximations\n",
    "    large_kappa_threshold = 1e5  # Threshold for considering kappa as \"large\"\n",
    "    small_kappa_threshold = 1e-3  # Threshold for considering kappa as \"small\"\n",
    "\n",
    "    # Approximate normalization constant for large and small kappa values\n",
    "\n",
    "\n",
    "    norm_const = torch.where(\n",
    "        kappas > large_kappa_threshold,\n",
    "        kappas / (2 * math.pi),  # Approximation for large kappa\n",
    "        kappas / (2 * math.pi * (1-torch.exp(-2*kappas)))\n",
    "    )\n",
    "    # norm_const = kappas / (4 * math.pi * (1-torch.exp(-2*kappas)))\n",
    "\n",
    "    # Compute dot products between input w and the axes of the spheres (unit vectors)\n",
    "    dot_products = torch.matmul(w, axes.transpose(0, 1))-1  # Shape: (data_sizes, num_spheres)\n",
    "\n",
    "    # Compute the weighted von Mises-Fisher pdf values\n",
    "    weighted_exps = weights * norm_const * torch.exp(kappas * dot_products)  # Shape: (data_sizes, num_spheres)\n",
    "    q = torch.sum(weighted_exps, dim=-1)  # Shape: (data_sizes,)\n",
    "    q = torch.clamp(q, min=1e-10, max=1e10)  # Further clamping to avoid extreme values\n",
    "    return q\n",
    "\n",
    "# KL 散度损失函数\n",
    "def kl_divergence_loss(weights, axes, kappas, raw_data, w, p, **kwargs):\n",
    "\n",
    "    kl_lambda = kwargs['kl_lambda']\n",
    "    l1_lambda = kwargs['l1_lambda']\n",
    "    l2_lambda = kwargs['l2_lambda']\n",
    "    epoch_step = kwargs['epoch_step']\n",
    "    # if epoch_step < 500:\n",
    "    #     kl_lambda = 0.0\n",
    "\n",
    "    total_prob = multi_vmf(weights, axes, kappas, raw_data)\n",
    "    kl_loss = -torch.log(total_prob + 1e-10).mean() if kl_lambda > 0 else torch.tensor(0.0)\n",
    "\n",
    "    q = multi_vmf(weights, axes, kappas, w)\n",
    "    # nonzero = p > 0\n",
    "    # area = 4 * math.pi / (weights.shape[0])\n",
    "    # kl_loss = -torch.sum(p[nonzero] * torch.log(q/p[nonzero])) * area if kl_lambda > 0 else torch.tensor(0.0)\n",
    "    rec_loss = torch.abs(q - p).mean() if l1_lambda > 0 else torch.tensor(0.0)\n",
    "    l2_loss = torch.norm(q - p, p=2).mean() if l2_lambda > 0 else torch.tensor(0.0)\n",
    "    loss = kl_lambda * kl_loss + l1_lambda * rec_loss + l2_lambda * l2_loss\n",
    "    loss_dict = {'KL': kl_loss.item(), 'Rec': rec_loss.item(), 'L2': l2_loss.item()}\n",
    "    return loss, loss_dict\n",
    "\n",
    "def smooth_curve(values, smoothing_factor=0.9):\n",
    "    smoothed_values = []\n",
    "    last = values[0]\n",
    "    for value in values:\n",
    "        smoothed_value = last * smoothing_factor + (1 - smoothing_factor) * value\n",
    "        smoothed_values.append(smoothed_value)\n",
    "        last = smoothed_value\n",
    "    return smoothed_values\n",
    "def plot_losses(train_losses, val_losses = None):\n",
    "    train_losses_smoothed = smooth_curve(train_losses)\n",
    "    if val_losses is not None:\n",
    "        val_losses_smoothed = smooth_curve(val_losses)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses_smoothed, label=\"Training Loss (Smoothed)\", color=\"blue\")\n",
    "    if val_losses is not None:\n",
    "        plt.plot(val_losses_smoothed, label=\"Validation Loss (Smoothed)\", color=\"red\")\n",
    "    plt.yscale(\"log\")  # Log scale for the y-axis\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (Log Scale)\")\n",
    "    plt.title(\"Loss (Log Scale with Smoothing)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_outputs_ed(encoder,decoder, all_ray_data, X, sizes, device):\n",
    "    \"\"\"\n",
    "    Plot the output X for each graph data in the dataset as a heatmap, and plot all ray data in the same figure.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained model.\n",
    "    - all_graph_data (list): List of graph data to be used for generating outputs.\n",
    "    - all_ray_data (list): List of ray data corresponding to each graph data.\n",
    "    - X (torch.Tensor): Prepared input data on the unit sphere.\n",
    "    - device (torch.device): Device to run the model on.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set the model to evaluation mode\n",
    "    decoder.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Define the ranges for theta and phi\n",
    "    z_min, z_max = -1, 1\n",
    "    phi_min, phi_max = -np.pi, np.pi\n",
    "\n",
    "    # Define tick positions in radians\n",
    "    z_ticks_pos = np.linspace(z_min, z_max, 5)        # -1, -0.5, 0, 0.5, 1\n",
    "    phi_ticks = np.linspace(phi_min, phi_max, 5)      # -pi, -pi/2, 0, pi/2, pi\n",
    "\n",
    "    # Convert tick positions to degrees for labeling\n",
    "    z_tick_pos_labels = [f\"{z:.2f}\" for z in z_ticks_pos]\n",
    "    phi_tick_labels = [f\"{int(np.degrees(p))}°\" for p in phi_ticks]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, ray_data in zip(file_idx, all_ray_data):\n",
    "            ray_data = ray_data.unsqueeze(0).unsqueeze(0) \n",
    "            ray_data = ray_data.to(device)\n",
    "\n",
    "            target_img = ray_data.cpu().numpy().reshape(sizes[0], sizes[1])\n",
    "\n",
    "            # 编码器：提取 embedding\n",
    "            embedding = encoder(ray_data)\n",
    "\n",
    "            # 解码器：生成权重、轴、kappa 参数\n",
    "            weights, axes, kappas = decoder(embedding)\n",
    "\n",
    "\n",
    "            # Compute the multi-vMF output for visualization\n",
    "            predict_img = multi_vmf(weights, axes, kappas, X).cpu().numpy()  # Convert to numpy for plotting\n",
    "\n",
    "            # Reshape X_output to match heatmap dimensions\n",
    "            try:\n",
    "                predict_img = predict_img.reshape((sizes[0], sizes[1]))\n",
    "            except ValueError:\n",
    "                print(f\"Error: Cannot reshape output for graph {i}. Check dimensions of X_output.\")\n",
    "                continue\n",
    "\n",
    "            # Set up subplots\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))  # Two subplots in one row\n",
    "\n",
    "            # Plot heatmap for the multi-vMF output on the first axis\n",
    "            im1 = ax1.imshow(\n",
    "                predict_img,\n",
    "                extent=[phi_min, phi_max, z_min, z_max],\n",
    "                aspect='auto',\n",
    "                origin='lower',\n",
    "                cmap='viridis'  # Adding color map to make visualization better\n",
    "            )\n",
    "            ax1.set_title(f'Prediction for Graph {i}')\n",
    "            ax1.set_xlabel('Phi')\n",
    "            ax1.set_ylabel('Z')\n",
    "\n",
    "            # Set x and y ticks for the first subplot\n",
    "            ax1.set_xticks(phi_ticks)\n",
    "            ax1.set_yticks(z_ticks_pos)\n",
    "\n",
    "            # Set tick labels in degrees for the first subplot\n",
    "            ax1.set_xticklabels(phi_tick_labels)\n",
    "            ax1.set_yticklabels(z_tick_pos_labels)\n",
    "            \n",
    "            # Enable ticks on top and right for the first subplot\n",
    "            ax1.tick_params(top=True, right=True, labeltop=True, labelright=True)\n",
    "            \n",
    "            # Add colorbar to the heatmap\n",
    "            fig.colorbar(im1, ax=ax1, orientation='vertical')\n",
    "\n",
    "            im2 = ax2.imshow(\n",
    "                target_img,\n",
    "                extent=[phi_min, phi_max, z_min, z_max],\n",
    "                aspect='auto',\n",
    "                origin='lower',\n",
    "                cmap='viridis'  # Different color map for better differentiation\n",
    "            )\n",
    "            ax2.set_title(f'Refernece for Graph {i}')\n",
    "            ax2.set_xlabel('Phi')\n",
    "            ax2.set_ylabel('Z')\n",
    "\n",
    "            # Set x and y ticks for the second subplot\n",
    "            ax2.set_xticks(phi_ticks)\n",
    "            ax2.set_yticks(z_ticks_pos)\n",
    "\n",
    "            # Set tick labels in degrees for the second subplot\n",
    "            ax2.set_xticklabels(phi_tick_labels)\n",
    "            ax2.set_yticklabels(z_tick_pos_labels)\n",
    "            \n",
    "            # Enable ticks on top and right for the second subplot\n",
    "            ax2.tick_params(top=True, right=True, labeltop=True, labelright=True)\n",
    "            \n",
    "            # Add colorbar to the ray data heatmap\n",
    "            fig.colorbar(im2, ax=ax2, orientation='vertical')\n",
    "\n",
    "            # Save figure\n",
    "            # plt.savefig(f'data/cnn/output_{i}.png')\n",
    "            # plt.close(fig)  # Close figure to free memory\n",
    "            plt.show()\n",
    "\n",
    "    encoder.train()  # Set the model back to training mode\n",
    "    decoder.train()  # Set the model back to training mode\n",
    "\n",
    "def plot_outputs_3d_ed(encoder, decoder, all_ray_data, X, sizes, device):\n",
    "    \"\"\"\n",
    "    Plot the output X for each graph data in the dataset as a 3D surface plot, and plot all ray data in the same figure.\n",
    "    \n",
    "    Parameters:\n",
    "    - encoder: The trained encoder model.\n",
    "    - decoder: The trained decoder model.\n",
    "    - all_ray_data (list): List of ray data corresponding to each graph data.\n",
    "    - X (torch.Tensor): Prepared input data on the unit sphere.\n",
    "    - device (torch.device): Device to run the model on.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set the model to evaluation mode\n",
    "    decoder.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Define the ranges for theta and phi\n",
    "    z_min, z_max = -1, 1\n",
    "    phi_min, phi_max = -np.pi, np.pi\n",
    "\n",
    "    # Create meshgrid for 3D plotting\n",
    "    z_in = np.linspace(-1, 1, sizes[0])\n",
    "    theta_in=np.linspace(-np.pi, np.pi, sizes[1])\n",
    "\n",
    "    Z, Phi = np.meshgrid(z_in, theta_in, indexing='ij')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, ray_data in enumerate(all_ray_data):\n",
    "            ray_data = ray_data.unsqueeze(0).unsqueeze(0) \n",
    "            ray_data = ray_data.to(device)\n",
    "\n",
    "            target_img = ray_data.cpu().numpy().reshape(sizes[0], sizes[1])\n",
    "\n",
    "            # Encoder: extract embedding\n",
    "            embedding = encoder(ray_data)\n",
    "\n",
    "            # Decoder: generate weights, axes, kappa parameters\n",
    "            weights, axes, kappas = decoder(embedding)\n",
    "\n",
    "            # Compute the multi-vMF output for visualization\n",
    "            predict_img = multi_vmf(weights, axes, kappas, X).cpu().numpy()  # Convert to numpy for plotting\n",
    "\n",
    "            # Reshape X_output to match heatmap dimensions\n",
    "            try:\n",
    "                predict_img = predict_img.reshape((sizes[0], sizes[1]))\n",
    "            except ValueError:\n",
    "                print(f\"Error: Cannot reshape output for graph {i}. Check dimensions of X_output.\")\n",
    "                continue\n",
    "\n",
    "            # Set up subplots for 3D visualization\n",
    "            fig = plt.figure(figsize=(14, 6))\n",
    "            ax1 = fig.add_subplot(121, projection='3d')\n",
    "            ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "            # Plot 3D surface for the multi-vMF output\n",
    "            ax1.plot_surface(Z, Phi, predict_img, rstride=1, cstride=1, cmap='rainbow')\n",
    "            ax1.set_title(f'Prediction for Graph {i}')\n",
    "            ax1.set_xlabel('Z')\n",
    "            ax1.set_ylabel('Phi')\n",
    "            ax1.set_zlabel('Value')\n",
    "\n",
    "            # Plot 3D surface for the target ray data\n",
    "            ax2.plot_surface(Z, Phi, target_img, rstride=1, cstride=1, cmap='rainbow')\n",
    "            ax2.set_title(f'Reference for Graph {i}')\n",
    "            ax2.set_xlabel('Z')\n",
    "            ax2.set_ylabel('Phi')\n",
    "            ax2.set_zlabel('Value')\n",
    "\n",
    "            # Show the figure\n",
    "            plt.show()\n",
    "\n",
    "    encoder.train()  # Set the model back to training mode\n",
    "    decoder.train()  # Set the model back to training mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1010853, 4)\n",
      "(4096, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\runze\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 数据加载\n",
    "data_dir = 'D:/Downloads/foamGNN/raw'\n",
    "sizes = np.array([32, 64, 32, 64])\n",
    "all_ray_data = []\n",
    "\n",
    "file_idx = [1]\n",
    "\n",
    "# for i in file_idx:\n",
    "#     rayfile = os.path.join(data_dir, f'{i}/stdata.bin')\n",
    "#     ray_data = load_ray_data(rayfile, sizes)\n",
    "#     # ray_data = torch.log(ray_data + 1)\n",
    "#     all_ray_data.append(ray_data)\n",
    "\n",
    "datafile = \"D:/Github/datasets/raw_data/foam0/2/stdataNonSpe.bin\"\n",
    "ray_data = load_ray_data(datafile, sizes)\n",
    "all_ray_data = [ray_data]\n",
    "\n",
    "# 准备输入数据 X\n",
    "i_idx = torch.arange(sizes[0], dtype=torch.float, device=device) / sizes[0]\n",
    "j_idx = torch.arange(sizes[1], dtype=torch.float, device=device) / sizes[1]\n",
    "i_grid, j_grid = torch.meshgrid(i_idx, j_idx)\n",
    "pos_x = i_grid * 2 - 1\n",
    "pos_phi = (j_grid * 2 - 1) * np.pi\n",
    "pos_r = torch.sqrt(1 - pos_x**2)\n",
    "pos_y = pos_r * torch.cos(pos_phi)\n",
    "pos_z = pos_r * torch.sin(pos_phi)\n",
    "X = torch.stack((pos_x, pos_y, pos_z), dim=-1).reshape(-1, 3).to(device)\n",
    "\n",
    "\n",
    "filename = \"D:/Github/datasets/raw_data/foam0/2/rawdataNonSpe.bin\"\n",
    "rawdata = np.fromfile(filename, dtype=np.float32)\n",
    "rawdata = rawdata.reshape(-1, 4)\n",
    "print(rawdata.shape)\n",
    "x = rawdata[:,0]\n",
    "# print(np.max(data[:,1]))\n",
    "phi = rawdata[:,1]-np.pi\n",
    "r = np.sqrt(1 - x**2)\n",
    "y = r * np.cos(phi)\n",
    "z = r * np.sin(phi)\n",
    "\n",
    "\n",
    "raw_X = np.column_stack((x, y, z))\n",
    "raw_num = min(4096, raw_X.shape[0])\n",
    "raw_X = raw_X[:raw_num, :]\n",
    "print(raw_X.shape)\n",
    "\n",
    "# 将数据转换为张量\n",
    "raw_data = torch.tensor(raw_X, dtype=torch.float32, device=device)\n",
    "all_raw_data = [raw_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "num_epochs = 5000\n",
    "kl_lambda = 1\n",
    "l1_lambda = 10\n",
    "l2_lambda = 1\n",
    "\n",
    "# 模型定义\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_spheres = 64\n",
    "\n",
    "init_lr = 5e-4\n",
    "decay_rate = 0.9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型（编码器和解码器）和优化器\n",
    "encoder = EncoderResNet(embedding_dim=embedding_dim).to(device)\n",
    "decoder = SphereGaussianMixture(embedding_dim=embedding_dim, hidden_dim=hidden_dim, num_spheres=num_spheres).to(device)\n",
    "\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)\n",
    "# scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)\n",
    "\n",
    "\n",
    "# 记录损失值\n",
    "loss_history = []\n",
    "l1_loss_history = []\n",
    "w_data =  X.clone().detach().to(device)\n",
    "X_data = raw_data.clone().detach().to(device)\n",
    "# 开始训练并使用 tqdm 显示进度\n",
    "with tqdm(total=num_epochs, desc=\"Training Progress\") as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        epoch_loss = 0.0\n",
    "        kl_loss = 0\n",
    "        rec_loss = 0\n",
    "        l2_loss = 0\n",
    "        file_count = 0\n",
    "        for idx, ray_data, raw_data in zip(file_idx, all_ray_data, all_raw_data):\n",
    "            # 对 ray_data 进行处理以适应编码器输入\n",
    "            ray_data = ray_data.unsqueeze(0).unsqueeze(0)  # 添加 batch 和 channel 维度 (1, 1, sizes[0], sizes[1])\n",
    "            ray_data = ray_data.to(device)\n",
    "\n",
    "            target_dist = ray_data.reshape(-1).to(device)\n",
    "\n",
    "            # 编码器：提取 embedding\n",
    "            embedding = encoder(ray_data)\n",
    "\n",
    "            # 解码器：生成权重、轴、kappa 参数\n",
    "            weights, axes, kappas = decoder(embedding)\n",
    "\n",
    "            # 计算损失\n",
    "            loss, loss_dict = kl_divergence_loss(weights, axes, kappas, raw_data, w_data, target_dist,\n",
    "                                                    kl_lambda=kl_lambda, l1_lambda=l1_lambda, l2_lambda=l2_lambda,\n",
    "                                                    epoch_step=epoch)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "\n",
    "            # 累计损失\n",
    "            epoch_loss += loss.item()\n",
    "            kl_loss += loss_dict['KL']\n",
    "            rec_loss += loss_dict['Rec']\n",
    "            l2_loss += loss_dict['L2']\n",
    "            file_count += 1\n",
    "\n",
    "        # Calculate and record the average loss for the epoch\n",
    "        avg_loss = epoch_loss / file_count\n",
    "        avg_kl_loss = kl_loss / file_count\n",
    "        avg_rec_loss = rec_loss / file_count\n",
    "        avg_l2_loss = l2_loss / file_count\n",
    "        loss_history.append(avg_loss)\n",
    "        l1_loss_history.append(avg_rec_loss)\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f\"{avg_loss:.6f}\", \n",
    "            'KL': f\"{avg_kl_loss:.6f}\", \n",
    "            'Rec': f\"{avg_rec_loss:.6f}\", \n",
    "            'L2': f\"{avg_l2_loss:.6f}\"\n",
    "        })\n",
    "        pbar.update(1)\n",
    "# 绘制损失率的衰减图\n",
    "plot_losses(l1_loss_history) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the output heatmaps for each graph data\n",
    "plot_outputs_ed(encoder, decoder, all_ray_data, X, sizes,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the output 3D surfaces for each graph data\n",
    "plot_outputs_3d_ed(encoder, decoder, all_ray_data, X, sizes,device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbgnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
