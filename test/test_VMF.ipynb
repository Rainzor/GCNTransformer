{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import threading\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 设置设备（如果有GPU则使用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vMFMixtureModel(nn.Module):\n",
    "    def __init__(self, num_components):\n",
    "        super(vMFMixtureModel, self).__init__()\n",
    "        self.num_components = num_components\n",
    "\n",
    "        # 混合系数的对数几率\n",
    "        self.w_logits = nn.Parameter(torch.randn(num_components))\n",
    "\n",
    "        # 角度参数 theta 和 phi\n",
    "        self.theta_phi = nn.Parameter(torch.randn(2 * num_components)*3)\n",
    "\n",
    "        # 浓度参数的对数\n",
    "        self.log_kappa = nn.Parameter(torch.randn(num_components))\n",
    "\n",
    "    def forward(self):\n",
    "        # 计算混合系数\n",
    "        weight = F.softmax(self.w_logits, dim=0)\n",
    "\n",
    "        # 使用 sigmoid 将 theta 和 phi 限制在 [0,1]\n",
    "        theta_phi = torch.sigmoid(self.theta_phi)\n",
    "\n",
    "        # 映射到实际角度范围\n",
    "        theta = theta_phi[:self.num_components] * math.pi          # [0, π]\n",
    "        phi = theta_phi[self.num_components:] * 2 * math.pi        # [0, 2π]\n",
    "\n",
    "        # 计算球坐标系下的 μ\n",
    "        cos_theta = torch.cos(theta)\n",
    "        sin_theta = torch.sin(theta)\n",
    "        cos_phi = torch.cos(phi)\n",
    "        sin_phi = torch.sin(phi)\n",
    "        mu = torch.stack((sin_theta * cos_phi, sin_theta * sin_phi, cos_theta), dim=1)  # Shape: (num_components, 3)\n",
    "\n",
    "        # 计算浓度参数\n",
    "        kappa = torch.exp(self.log_kappa)\n",
    "        # kappa = torch.clamp(kappa, min=1e-10, max=1e5)  # 限制 kappa 的最大值，防止数值不稳定\n",
    "\n",
    "        return weight, mu, kappa\n",
    "    \n",
    "    \n",
    "\n",
    "def negative_log_likelihood(raw_data, weight, mu, kappa, **kwargs):\n",
    "\n",
    "    kl_lambda = kwargs['kl_lambda']\n",
    "    l1_lambda = kwargs['l1_lambda']\n",
    "    l2_lambda = kwargs['l2_lambda']\n",
    "    p = kwargs['p']\n",
    "    w = kwargs['w']\n",
    "\n",
    "    total_prob = multi_vmf(weight, mu, kappa, raw_data)\n",
    "    if kl_lambda > 0:\n",
    "        nll = -torch.log(total_prob + 1e-10).mean()\n",
    "    else:\n",
    "        nll = torch.tensor(0.0)\n",
    "\n",
    "    q = multi_vmf(weight, mu, kappa, w)\n",
    "    if l1_lambda > 0:\n",
    "        rec_loss = torch.abs(q - p).mean()\n",
    "    else:\n",
    "        rec_loss = torch.tensor(0.0)\n",
    "\n",
    "    if l2_lambda > 0:\n",
    "        l2_loss = torch.norm(q - p, p=2).mean()\n",
    "    else:\n",
    "        l2_loss = torch.tensor(0.0)\n",
    "\n",
    "    loss = kl_lambda * nll + l1_lambda * rec_loss + l2_lambda * l2_loss\n",
    "    return loss, {'NLL': nll, 'Rec': rec_loss, 'L2': l2_loss}\n",
    "\n",
    "# 多重 von Mises-Fisher 分布函数\n",
    "def multi_vmf(weights, axes, kappas, w):\n",
    "    # Ensure kappas are non-negative for stability\n",
    "    kappas = torch.clamp(kappas, min=1e-10, max=1e5)\n",
    "\n",
    "    # Define thresholds for approximations\n",
    "    large_kappa_threshold = 1e5  # Threshold for considering kappa as \"large\"\n",
    "    small_kappa_threshold = 1e-3  # Threshold for considering kappa as \"small\"\n",
    "\n",
    "    # Approximate normalization constant for large and small kappa values\n",
    "\n",
    "\n",
    "    norm_const = torch.where(\n",
    "        kappas > large_kappa_threshold,\n",
    "        kappas / (2 * math.pi),  # Approximation for large kappa\n",
    "        kappas / (2 * math.pi * (1-torch.exp(-2*kappas)))\n",
    "    )\n",
    "    # norm_const = kappas / (4 * math.pi * (1-torch.exp(-2*kappas)))\n",
    "\n",
    "    # Compute dot products between input w and the axes of the spheres (unit vectors)\n",
    "    dot_products = torch.matmul(w, axes.transpose(0, 1))-1  # Shape: (data_sizes, num_spheres)\n",
    "\n",
    "    # Compute the weighted von Mises-Fisher pdf values\n",
    "    weighted_exps = weights * norm_const * torch.exp(kappas * dot_products)  # Shape: (data_sizes, num_spheres)\n",
    "    q = torch.sum(weighted_exps, dim=-1)  # Shape: (data_sizes,)\n",
    "    q = torch.clamp(q, min=1e-10, max=1e10)  # Further clamping to avoid extreme values\n",
    "    return q\n",
    "\n",
    "def smooth_curve(values, smoothing_factor=0.9):\n",
    "    smoothed_values = []\n",
    "    last = values[0]\n",
    "    for value in values:\n",
    "        smoothed_value = last * smoothing_factor + (1 - smoothing_factor) * value\n",
    "        smoothed_values.append(smoothed_value)\n",
    "        last = smoothed_value\n",
    "    return smoothed_values\n",
    "\n",
    "def plot_losses(train_losses, val_losses = None):\n",
    "    train_losses_smoothed = smooth_curve(train_losses)\n",
    "    if val_losses is not None:\n",
    "        val_losses_smoothed = smooth_curve(val_losses)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses_smoothed, label=\"Training Loss (Smoothed)\", color=\"blue\")\n",
    "    if val_losses is not None:\n",
    "        plt.plot(val_losses_smoothed, label=\"Validation Loss (Smoothed)\", color=\"red\")\n",
    "    plt.yscale(\"log\")  # Log scale for the y-axis\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (Log Scale)\")\n",
    "    plt.title(\"Loss (Log Scale with Smoothing)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_outputs_3d(references, predictions, sizes, save_path=None,title=\"\"):\n",
    "    # 定义 z 和 phi 的范围\n",
    "    z_min, z_max = -1, 1\n",
    "    phi_min, phi_max = -np.pi, np.pi\n",
    "\n",
    "    # 创建用于 3D 绘图的网格\n",
    "    z_in = np.linspace(z_min, z_max, sizes[0])\n",
    "    phi_in = np.linspace(phi_min, phi_max, sizes[1])\n",
    "\n",
    "    Z, Phi = np.meshgrid(z_in, phi_in, indexing='ij')\n",
    "\n",
    "    target_img = references\n",
    "    predict_img = predictions\n",
    "\n",
    "    # 确保输入数据的形状与网格匹配\n",
    "    if predict_img.shape != Z.shape:\n",
    "        predict_img = predict_img.reshape(Z.shape)\n",
    "    if target_img is not None and target_img.shape != Z.shape:\n",
    "        target_img = target_img.reshape(Z.shape)\n",
    "\n",
    "    # 设置用于 3D 可视化的子图\n",
    "    if target_img is not None and np.sum(target_img) > 0:\n",
    "        fig = plt.figure(figsize=(14, 6))\n",
    "        ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "        ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        ax1 = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "\n",
    "    # 绘制预测数据的 3D 曲面\n",
    "    ax1.plot_surface(Z, Phi, predict_img, rstride=1, cstride=1, cmap='rainbow')\n",
    "    ax1.set_title(f'Prediction {title}')\n",
    "    ax1.set_xlabel('Z')\n",
    "    ax1.set_ylabel('Phi')\n",
    "    ax1.set_zlabel('Value')\n",
    "\n",
    "    # 如果有参考数据，绘制其 3D 曲面\n",
    "    if target_img is not None and np.sum(target_img) > 0:\n",
    "        ax2.plot_surface(Z, Phi, target_img, rstride=1, cstride=1, cmap='rainbow')\n",
    "        ax2.set_title(f'Reference {title}')\n",
    "        ax2.set_xlabel('Z')\n",
    "        ax2.set_ylabel('Phi')\n",
    "        ax2.set_zlabel('Value')\n",
    "\n",
    "    # 显示图形\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "def get_gridX(sizes):\n",
    "    i_idx = torch.arange(sizes[0], dtype=torch.float, device=device) / sizes[0]\n",
    "    j_idx = torch.arange(sizes[1], dtype=torch.float, device=device) / sizes[1]\n",
    "    i_grid, j_grid = torch.meshgrid(i_idx, j_idx)\n",
    "    pos_x = i_grid * 2 - 1\n",
    "    pos_phi = (j_grid * 2 - 1) * np.pi\n",
    "    pos_r = torch.sqrt(1 - pos_x**2)\n",
    "    pos_y = pos_r * torch.cos(pos_phi)\n",
    "    pos_z = pos_r * torch.sin(pos_phi)\n",
    "    X = torch.stack((pos_x, pos_y, pos_z), dim=-1).reshape(-1, 3).to(device)\n",
    "    return X   \n",
    "\n",
    "# 加载 ray 数据的函数\n",
    "def load_rawdata(filename, sizes, device, verbose=False, dtype=np.float32):\n",
    "    X = get_gridX(sizes)    \n",
    "    # 加载数据\n",
    "    rawdata = np.fromfile(filename, dtype=dtype)\n",
    "    \n",
    "    # 如果是 float16，转换为 float32\n",
    "    if dtype == np.float16:\n",
    "        if verbose:\n",
    "            print(f\"Converting data from float16 to float32\")\n",
    "        rawdata = rawdata.astype(np.float32)\n",
    "    rawdata = rawdata.reshape(-1, 4)\n",
    "    # print(rawdata.shape)\n",
    "    x = rawdata[:,0]\n",
    "    # print(np.max(data[:,1]))\n",
    "    phi = rawdata[:,1]-np.pi\n",
    "    r = np.sqrt(1 - x**2)\n",
    "    y = r * np.cos(phi)\n",
    "    z = r * np.sin(phi)\n",
    "\n",
    "\n",
    "    # 创建网格分布\n",
    "    x_edges = np.linspace(-1, 1, sizes[0]+1)  \n",
    "    phi_edges = np.linspace(-np.pi, np.pi, sizes[1]+1)\n",
    "\n",
    "    # 统计 (X, φ) 分布\n",
    "    H, _, _ = np.histogram2d(x, phi, bins=[x_edges, phi_edges])\n",
    "    ray_data = torch.tensor(H, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "    if ray_data.shape[0] != sizes[0] * sizes[1]:\n",
    "        print(\"Error: ray data shape mismatch!\")\n",
    "        exit(1)\n",
    "    area = 4 * math.pi / (ray_data.shape[0])\n",
    "    ray_data = ray_data / torch.sum(ray_data) / area\n",
    "    \n",
    "\n",
    "    raw_X = np.column_stack((x, y, z))\n",
    "    np.random.shuffle(raw_X)\n",
    "    raw_num = min(8192, raw_X.shape[0])\n",
    "    raw_X = raw_X[:raw_num, :]\n",
    "\n",
    "    # 将数据转换为张量\n",
    "    raw_data = torch.tensor(raw_X, dtype=torch.float32, device=device)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"X Mesh shape:\", X.shape)\n",
    "        print(\"ray data shape:\", ray_data.shape)\n",
    "        print(\"raw data shape:\", raw_data.shape)\n",
    "    return raw_data, ray_data, X\n",
    "\n",
    "# 模型训练函数\n",
    "def train_model(model_id, vmf, optimizer, dataset, hyperparams, device, save_path=None, verbose=False):\n",
    "    kl_lambda = hyperparams['kl_lambda']\n",
    "    l1_lambda = hyperparams['l1_lambda']\n",
    "    l2_lambda = hyperparams['l2_lambda']\n",
    "    num_epochs = hyperparams['num_epochs']\n",
    "\n",
    "    # 将数据移动到指定设备\n",
    "    samples = dataset[\"samples\"].to(device, non_blocking=True)\n",
    "    target = dataset[\"target\"].to(device, non_blocking=True)\n",
    "    w_data = dataset[\"w_data\"].to(device, non_blocking=True)\n",
    "\n",
    "    def update_model():\n",
    "        optimizer.zero_grad()\n",
    "        pi, mu, kappa = vmf()\n",
    "        loss, loss_dict = negative_log_likelihood(\n",
    "            samples, pi, mu, kappa,\n",
    "            kl_lambda=kl_lambda, l1_lambda=l1_lambda, l2_lambda=l2_lambda,\n",
    "            p=target, w=w_data\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss, loss_dict\n",
    "\n",
    "    vmf.train()\n",
    "\n",
    "    if verbose:\n",
    "        loss_history = []\n",
    "        with tqdm(total=num_epochs, desc=f'Model {model_id} Training on {device}') as pbar:\n",
    "            for epoch in range(num_epochs):\n",
    "                loss, loss_dict = update_model()\n",
    "                loss_history.append(loss.item())\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'KL': f'{loss_dict[\"NLL\"].item():.4f}',\n",
    "                    'Rec': f'{loss_dict[\"Rec\"].item():.4f}',\n",
    "                    'L2': f'{loss_dict[\"L2\"].item():.4f}'\n",
    "                })\n",
    "                pbar.update(1)\n",
    "\n",
    "        plot_losses(loss_history)\n",
    "    else:\n",
    "        for epoch in range(num_epochs):\n",
    "            update_model()\n",
    "\n",
    "    # 保存模型参数\n",
    "    if save_path is not None:\n",
    "        model_save_path = os.path.join(save_path, f\"vmf_parameters.pth\")\n",
    "        torch.save(vmf.state_dict(), model_save_path)\n",
    "        print(f\"Model {model_id} parameters saved to {model_save_path}\")\n",
    "\n",
    "# 加载模型参数\n",
    "def load_model(model_path, num_components, device):\n",
    "    # 初始化模型\n",
    "    model = vMFMixtureModel(num_components=num_components).to(device)\n",
    "    \n",
    "    # 加载保存的参数\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # 切换到评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = np.array([64,64])\n",
    "base_path = \"D:/Github/datasets/raw_data_more/foam0/4/\"\n",
    "filename = os.path.join(base_path,\"rawdata.bin\")\n",
    "raw_data, ray_data, X = load_rawdata(filename, sizes, device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型和优化器\n",
    "num_components = 64\n",
    "vmf = vMFMixtureModel(num_components=num_components).to(device)\n",
    "optimizer = torch.optim.Adam(vmf.parameters(), lr=5e-3, weight_decay=1e-5)\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 10000\n",
    "loss_history = []\n",
    "l1_loss_history = []\n",
    "kl_lambda = 1\n",
    "l1_lambda = 10\n",
    "l2_lambda = 0.5\n",
    "\n",
    "hyperparams = {\n",
    "    \"kl_lambda\":kl_lambda,\n",
    "    \"l1_lambda\":l1_lambda,\n",
    "    \"l2_lambda\":l2_lambda,\n",
    "    \"num_epochs\":num_epochs\n",
    "}\n",
    "\n",
    "# samples = raw_data.clone().detach()\n",
    "# target_dist = ray_data.reshape(-1).to(device)\n",
    "# w_data = X.clone().detach()\n",
    "dataset = {\n",
    "    \"samples\": raw_data.clone().detach(),\n",
    "    \"target\": ray_data.reshape(-1).to(device),\n",
    "    \"w_data\": X.clone().detach()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(0, vmf, optimizer, dataset, hyperparams, device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "with torch.no_grad():\n",
    "    vmf.eval()\n",
    "    weights, axes, kappas = vmf()\n",
    "    predictions = multi_vmf(weights, axes, kappas, X).cpu().numpy() \n",
    "    predictions = predictions.reshape(sizes[0], sizes[1])\n",
    "    reference = ray_data.cpu().numpy().reshape(sizes[0], sizes[1])\n",
    "    plot_outputs_3d(reference, predictions, sizes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_dense = np.array([128,128])\n",
    "X_dense = get_gridX(sizes_dense)\n",
    "with torch.no_grad():\n",
    "    vmf.eval()\n",
    "    weights, axes, kappas = vmf()\n",
    "    predictions = multi_vmf(weights, axes, kappas, X_dense).cpu().numpy() \n",
    "    predictions = predictions.reshape(sizes_dense)\n",
    "    reference = np.zeros_like(predictions)\n",
    "    plot_outputs_3d(reference, predictions, sizes_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    vmf.eval()\n",
    "    weights, axes, kappas = vmf()\n",
    "    predictions = multi_vmf(weights, axes, kappas, X).cpu().numpy() \n",
    "    predictions = predictions.reshape(sizes[0], sizes[1])\n",
    "    reference = ray_data.cpu().numpy().reshape(sizes[0], sizes[1])\n",
    "    error = np.abs(predictions - reference)\n",
    "    zero_e = np.zeros_like(error)\n",
    "    plot_outputs_3d(zero_e, error, sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型和优化器参数\n",
    "num_components = 64\n",
    "num_epochs = 10000\n",
    "kl_lambda = 1\n",
    "l1_lambda = 10\n",
    "l2_lambda = 0.5\n",
    "learning_rate = 5e-3\n",
    "weight_decay = 1e-5\n",
    "sizes = np.array([64, 64])\n",
    "\n",
    "hyperparams = {\n",
    "    \"kl_lambda\":kl_lambda,\n",
    "    \"l1_lambda\":l1_lambda,\n",
    "    \"l2_lambda\":l2_lambda,\n",
    "    \"num_epochs\":num_epochs\n",
    "}\n",
    "\n",
    "# 数据文件路径（针对每个模型的不同数据集）\n",
    "\n",
    "def generate_filenames(base_path, num_files, sepcular=False):\n",
    "    \"\"\"生成文件路径列表\"\"\"\n",
    "    if sepcular:\n",
    "        return [os.path.join(base_path, f\"{i}\\\\rawdata.bin\") for i in range(0, num_files)]\n",
    "    else:\n",
    "        return [os.path.join(base_path, f\"{i}\\\\rawdataNonSpe.bin\") for i in range(0, num_files)]\n",
    "\n",
    "# 初始化参数\n",
    "base_path = \"D:\\\\Github\\\\datasets\\\\raw_data\\\\foam0\"\n",
    "num_files = 14  # 从 1 到 14\n",
    "filenames = generate_filenames(base_path, num_files)\n",
    "num_models = len(filenames)\n",
    "\n",
    "datasets = []  \n",
    "optimizers = [] \n",
    "models = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    vmf = vMFMixtureModel(num_components=num_components).to(device)\n",
    "    optimizer = torch.optim.Adam(vmf.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    models.append(vmf)\n",
    "    optimizers.append(optimizer)\n",
    "\n",
    "    # 加载对应的数据集\n",
    "    raw_data, ray_data, X = load_rawdata(filenames[i], sizes, device, verbose=False)\n",
    "    dataset = {\n",
    "        \"samples\": raw_data.clone().detach(),\n",
    "        \"target\": ray_data.reshape(-1).to(device),\n",
    "        \"w_data\": X.clone().detach()\n",
    "    }\n",
    "    datasets.append(dataset)\n",
    "\n",
    "\n",
    "# 确定保存路径（根据每个数据文件夹保存到相应位置）\n",
    "save_paths = [os.path.dirname(f) for f in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_models):\n",
    "    train_model(i, models[i], optimizers[i], datasets[i], hyperparams, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启动线程进行并行训练\n",
    "threads = []\n",
    "for i in range(num_models):\n",
    "    thread = threading.Thread(\n",
    "        target=train_model,\n",
    "        args=(\n",
    "            i + 1, models[i], optimizers[i],\n",
    "            datasets[i],\n",
    "            hyperparams,\n",
    "            device, save_paths[i]\n",
    "        )\n",
    "    )\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "# 等待所有线程结束\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_path = f\"D:\\\\Github\\\\PBGNN\\\\data\\\\firstpb\\\\raw\\\\firstpb1\\\\foam4\\\\1\\\\vmf_shell.pth\"\n",
    "num_components=64\n",
    "sizes = [64,64]\n",
    "model = load_model(params_path, num_components, device).to(device)\n",
    "data_path = f\"D:\\\\Github\\\\PBGNN\\\\data\\\\firstpb\\\\raw\\\\firstpb1\\\\foam4\\\\1\\\\rawdataAll0.bin\"\n",
    "raw_data, ray_data, X = load_rawdata(data_path, sizes, device, verbose=False, dtype=np.float16)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    weights, axes, kappas = model()\n",
    "    predictions = multi_vmf(weights, axes, kappas, X).cpu().numpy() \n",
    "    predictions = predictions.reshape(sizes[0], sizes[1])\n",
    "    reference = ray_data.cpu().numpy().reshape(sizes[0], sizes[1])\n",
    "    plot_outputs_3d(reference, predictions, sizes, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1]\n",
    "idx = 0\n",
    "params_path = f\"..\\\\data\\\\foam14FirstShell\\\\0\\\\vmf_parameters{idx}.pth\"\n",
    "num_components=64\n",
    "sizes = [64,64]\n",
    "model = load_model(params_path, num_components, device).to(device)\n",
    "data_path = f\"..\\\\data\\\\foam14FirstShell\\\\0\\\\rawdataNonSpe{idx}.bin\"\n",
    "raw_data, ray_data, X = load_rawdata(data_path, sizes, device, verbose=False, dtype=np.float16)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    weights, axes, kappas = model()\n",
    "    predictions = multi_vmf(weights, axes, kappas, X).cpu().numpy() \n",
    "    predictions = predictions.reshape(sizes[0], sizes[1])\n",
    "    reference = ray_data.cpu().numpy().reshape(sizes[0], sizes[1])\n",
    "    plot_outputs_3d(reference, predictions, sizes, save_path=None, title=f\"r={r[idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbgnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
