{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Helper function for visualization.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from models.utils import *\n",
    "from models.data_utils.transform import *\n",
    "from models.data_utils.dataset import FoamDataset\n",
    "from models.model import GraphTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例使用\n",
    "base_path = \"D:\\\\Github\\\\datasets\\\\\"\n",
    "foam_names = [\"foam0\", \"foam1\", \"foam2\", \"foam3\", \"foam4\", \"foam5\"]\n",
    "file_num = 14\n",
    "sizes = [64, 64]\n",
    "\n",
    "pre_transform = None\n",
    "\n",
    "rawdata = FoamDataset(root=base_path, foam_names=foam_names, file_num=file_num, sizes=sizes, pre_transform=pre_transform)\n",
    "print(rawdata[0].x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'num_features': rawdata[0].x.size(1),\n",
    "    'gnn_dim': 128,\n",
    "    'gnn_layers': 3,\n",
    "    'transformer_width': 256,\n",
    "    'transformer_layers': 4,\n",
    "    'transformer_heads': 8,\n",
    "    'hidden_dim': 512,\n",
    "    'output_dim': 256,\n",
    "    'embedding_dim': 32,\n",
    "    'pos_dim': rawdata[0].pos.size(1),\n",
    "    'dropout': 0.1,\n",
    "    'gnn_type': 'GCNConv',\n",
    "    'pool' : 'cls',\n",
    "    'patch_rw_dim': 16,\n",
    "\n",
    "    'num_epochs': 3000,\n",
    "    'lr': 1e-4,\n",
    "    'gamma': 0.9,\n",
    "    'step_size': 500,\n",
    "    'batch_size': 16,\n",
    "    'num_hops': 1,\n",
    "    'n_patches': 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = GraphPartitionTransform(\n",
    "    n_patches = hyperparameters['n_patches'],\n",
    "    metis = True,\n",
    "    drop_rate = 0.0,\n",
    "    num_hops = hyperparameters['num_hops'],\n",
    "    is_directed = False,\n",
    "    patch_rw_dim = hyperparameters['patch_rw_dim'],\n",
    "    patch_num_diff = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata.transform = transform\n",
    "\n",
    "datasets = [x for x in rawdata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in datasets[0].keys():\n",
    "    print(key, datasets[0][key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(datasets, batch_size=hyperparameters['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = next(iter(train_loader))\n",
    "for key in first.keys():\n",
    "    print(key, first[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_transformer = GraphTransformer(\n",
    "    num_features=hyperparameters['num_features'],\n",
    "    gnn_dim=hyperparameters['gnn_dim'],\n",
    "    gnn_layers=hyperparameters['gnn_layers'],\n",
    "    transformer_width=hyperparameters['transformer_width'],\n",
    "    transformer_layers=hyperparameters['transformer_layers'],\n",
    "    transformer_heads=hyperparameters['transformer_heads'],\n",
    "    hidden_dim=hyperparameters['hidden_dim'],\n",
    "    output_dim=hyperparameters['output_dim'],\n",
    "    embedding_dim=hyperparameters['embedding_dim'],\n",
    "    pos_dim=hyperparameters['pos_dim'],\n",
    "    dropout=hyperparameters['dropout'],\n",
    "    gnn_type=hyperparameters['gnn_type'],\n",
    "    pool=hyperparameters['pool'],\n",
    "    patch_rw_dim=hyperparameters['patch_rw_dim']\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())  # 总参数数量\n",
    "    return total_params\n",
    "\n",
    "total_params = count_parameters(graph_transformer)\n",
    "print(f\"Total params: {total_params}, {total_params / 1024 / 1024} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(graph_transformer.parameters(), lr=hyperparameters['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "import tqdm\n",
    "num_epochs = 1000\n",
    "with tqdm.tqdm(total=num_epochs) as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        graph_transformer.train()\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = graph_transformer(data).reshape(-1)\n",
    "            loss = F.mse_loss(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        loss_history.append(epoch_loss)\n",
    "        pbar.set_postfix({'loss': f\"{epoch_loss:.6f}\"})\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = rawdata[1]\n",
    "X = get_gridX(sizes, device=device)\n",
    "with torch.no_grad():\n",
    "    graph_transformer.eval()\n",
    "    weights, mus, kappas = graph_transformer.vmf_param(sample.to(device))\n",
    "    img_predict = multi_vmf(weights.squeeze(), mus.squeeze(), kappas.squeeze(), X).cpu().numpy()\n",
    "    img_predict = img_predict.reshape(sizes)\n",
    "\n",
    "    tgt_w, tgt_m, tgt_k = extract_param(sample.y)\n",
    "    img_reference = multi_vmf(tgt_w, tgt_m, tgt_k, X).cpu().numpy()\n",
    "    img_reference = img_reference.reshape(sizes)\n",
    "    plot_outputs_3d(img_reference, img_predict, sizes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbgnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
